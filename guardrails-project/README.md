# Guardrails Project

A practical framework to ensure safe, grounded, and policy-compliant interactions with large language models (LLMs).
This project implements input and output guardrails, a trust scoring system, and a decision policy engine to minimize risks such as hallucinations, jailbreaks, PII leakage, and policy violations.

## ğŸ¯ Overview

The Guardrails Project provides a multi-layer defense pipeline:

- **Input Guardrails** â€“ Detect jailbreaks, filter unsafe or PII-laden prompts
- **Retriever (optional)** â€“ Retrieve supporting evidence from a knowledge base
- **Output Guardrails** â€“ Check grounding, detect contradictions, assess relevance
- **Trust Score** â€“ Aggregate signals (relevance, coverage, consistency) into a confidence score
- **Decision Policy** â€“ Choose between accept, ask for clarification, citations-only, or block
- **Evaluation Harness** â€“ Benchmark performance on general and domain-specific datasets

## ğŸ—ï¸ Architecture

```
User Query â†’ Input Guardrails â†’ Retriever/Verifier â†’ LLM â†’ Output Guardrails â†’ Trust Score â†’ Decision Policy â†’ Final Response
```

## ğŸ“ Project Structure

```
guardrails-project/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ thresholds.yaml       # trust score thresholds, decision rules
â”‚   â”œâ”€â”€ datasets.yaml         # dataset paths & metadata
â”‚   â””â”€â”€ policies.yaml         # PII & policy configs
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ generic/              # e.g., HotpotQA subset
â”‚   â”œâ”€â”€ domain/               # enterprise handbook KB
â”‚   â”œâ”€â”€ pii/                  # synthetic PII set
â”‚   â””â”€â”€ jailbreak/            # jailbreak & red-team prompts
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ input_guardrails/     # jailbreak + pii filters
â”‚   â”œâ”€â”€ retriever/            # retrieval/fact checker
â”‚   â”œâ”€â”€ output_guardrails/    # grounding, contradiction, judge
â”‚   â”œâ”€â”€ trust_score/          # score aggregation
â”‚   â”œâ”€â”€ decision_policy/      # policy engine
â”‚   â”œâ”€â”€ evaluators/           # metrics: DeepEval, RAGAS
â”‚   â””â”€â”€ app/                  # API server / proxy
â”œâ”€â”€ notebooks/                # Jupyter analysis
â”‚   â”œâ”€â”€ exploration.ipynb
â”‚   â”œâ”€â”€ evaluation.ipynb
â”‚   â””â”€â”€ ablation.ipynb
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ redteam/
â””â”€â”€ results/
    â”œâ”€â”€ metrics/
    â”œâ”€â”€ dashboards/
    â””â”€â”€ reports/
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9+
- pip or conda
- OpenAI / HF model API key (optional for embeddings)

### Setup

```bash
git clone <repo-url>
cd guardrails-project
pip install -r requirements.txt
```

### Run Pipeline

```python
from src.app.guardrails_pipeline import GuardrailsPipeline

pipeline = GuardrailsPipeline(config_path="./config")

resp = pipeline.process_query("Who wrote Pride and Prejudice?")
print(resp.text, resp.trust_score, resp.decision)
```

## ğŸ”§ Configuration

### Trust Score Thresholds (`config/thresholds.yaml`)

```yaml
trust_score:
  high_confidence: 0.75   # Accept
  medium_confidence: 0.55 # Clarify or citations-only
  low_confidence: 0.40    # Block or escalate
```

### Policy Rules (`config/policies.yaml`)

```yaml
pii_filters:
  email: true
  phone: true
  credit_card: true
jailbreak_patterns:
  - "ignore previous"
  - "reveal secret"
```

### Dataset Config (`config/datasets.yaml`)

```yaml
datasets:
  generic: "data/generic/hotpotqa_subset.json"
  domain: "data/domain/enterprise_handbook.json"
  pii: "data/pii/pii_test.csv"
  jailbreak: "data/jailbreak/jailbreak_prompts.json"
```

## ğŸ§ª Evaluation

### Datasets

- **Generic** â€“ HotpotQA subset (factual QA grounding)
- **Domain** â€“ Enterprise handbook KB + 200 curated QAs
- **Stress Tests** â€“ Synthetic PII set (Faker), jailbreak prompt suite

### Metrics

- **Safety** â€“ jailbreak block rate, PII precision/recall
- **Quality** â€“ grounding score, contradiction risk, citation coverage
- **Performance** â€“ latency, cost overhead

Run evaluations:

```bash
jupyter notebook notebooks/evaluation.ipynb
```

## ğŸ§© Components

- **Input Guardrails**: `jailbreak_detector.py`, `pii_filter.py`
- **Retriever**: `vector_store.py` (ChromaDB or FAISS)
- **Output Guardrails**: `grounding_scorer.py`, `nli_checker.py`, `judge.py`
- **Trust Score**: `aggregator.py`, `thresholds.yaml`
- **Decision Policy**: `policy_engine.py` (accept / clarify / block)
- **Evaluators**: `ragas_eval.py`, `deepeval_runner.py`

## ğŸ“Š Deliverables

- Middleware proxy service with config-driven guardrails
- Evaluation harness with generic + domain datasets
- Metrics dashboard (grounding, hallucination reduction, block rates)
- Final report with ablations and thresholds

## âš ï¸ Notes

- Guardrails reduce but do not eliminate hallucinations and risks
- Domain datasets must be carefully curated for best results
- Always complement with human oversight in high-stakes domains

---

**Built with safety and reliability in mind** ğŸ›¡ï¸
{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ocqju7NbBxiK"
      },
      "source": [
        "# Generic Pipeline Demo (v2)\n",
        "_Generated 2025-10-02T03:09:05.772454Z_\n",
        "\n",
        "**What's new:** Fix retrieval for TruthfulQA by indexing **questions** with **TF‑IDF (1–2 grams, stopwords removed)**. This avoids the earlier Jaccard issue where generic answers like 'Nothing happens...' dominated.\n",
        "\n",
        "**Includes:** Extraction → Canonicalization (now captures `source` if present) → EDA (extra plots) → Improved retrieval demo with a real example (watermelon seeds)."
      ],
      "id": "Ocqju7NbBxiK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GGLMhJuBxiL"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os, json, zipfile, re\n",
        "from typing import List, Dict, Any\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "BASE = '/mnt/data'\n",
        "RAW_DIR = os.path.join(BASE, 'data', 'raw')\n",
        "PROC_DIR = os.path.join(BASE, 'data', 'processed')\n",
        "EDA_DIR = os.path.join(BASE, 'data', 'eda', 'figures')\n",
        "os.makedirs(RAW_DIR, exist_ok=True)\n",
        "os.makedirs(PROC_DIR, exist_ok=True)\n",
        "os.makedirs(EDA_DIR, exist_ok=True)\n",
        "\n",
        "# Uploaded sources in this environment\n",
        "UPLOADED_FEVER = '/mnt/data/train.jsonl'\n",
        "UPLOADED_TRUTH = '/mnt/data/archive.zip'\n",
        "\n",
        "FEVER_JSONL = os.path.join(RAW_DIR, 'fever_train.jsonl')\n",
        "TRUTH_ZIP   = os.path.join(RAW_DIR, 'truthfulqa_archive.zip')\n",
        "HOTPOT_DEV  = os.path.join(RAW_DIR, 'hotpotqa', 'dev.json')  # optional\n",
        "\n",
        "import shutil\n",
        "if os.path.exists(UPLOADED_FEVER) and not os.path.exists(FEVER_JSONL):\n",
        "    shutil.copy(UPLOADED_FEVER, FEVER_JSONL)\n",
        "if os.path.exists(UPLOADED_TRUTH) and not os.path.exists(TRUTH_ZIP):\n",
        "    shutil.copy(UPLOADED_TRUTH, TRUTH_ZIP)\n",
        "\n",
        "print('RAW_DIR:', RAW_DIR)\n",
        "print('Files present:', os.listdir(RAW_DIR))"
      ],
      "id": "8GGLMhJuBxiL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1Emol69BxiL"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def to_str_id(x):\n",
        "    try:\n",
        "        return str(x)\n",
        "    except Exception:\n",
        "        return str(hash(x))\n",
        "\n",
        "def ci_get(df: pd.DataFrame, *cands):\n",
        "    cols = {c.lower(): c for c in df.columns}\n",
        "    for c in cands:\n",
        "        if c.lower() in cols:\n",
        "            return cols[c.lower()]\n",
        "    return None\n",
        "\n",
        "def clean_text(s):\n",
        "    if s is None:\n",
        "        return None\n",
        "    if not isinstance(s, str):\n",
        "        s = str(s)\n",
        "    return s.strip()\n",
        "\n",
        "def write_jsonl(rows, path):\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        for r in rows:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + '\\n')\n",
        "\n",
        "CANONICAL_OUT = os.path.join(PROC_DIR, 'generic_canonical.jsonl')"
      ],
      "id": "w1Emol69BxiL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Im-q_coMBxiL"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def load_fever_canonical(path: str):\n",
        "    rows = []\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"[FEVER] Not found: {path}\")\n",
        "        return rows\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            try:\n",
        "                obj = json.loads(line)\n",
        "            except Exception:\n",
        "                continue\n",
        "            cid = to_str_id(obj.get('id', i))\n",
        "            claim = clean_text(obj.get('claim'))\n",
        "            label = obj.get('label')\n",
        "            evidence = obj.get('evidence', [])\n",
        "            rows.append({\n",
        "                'dataset': 'fever',\n",
        "                'id': cid,\n",
        "                'query': claim,\n",
        "                'answer': None,\n",
        "                'contexts': [],\n",
        "                'label': label,\n",
        "                'meta': {'evidence': evidence}\n",
        "            })\n",
        "    print(f'[FEVER] Canonicalized rows: {len(rows)}')\n",
        "    return rows\n",
        "\n",
        "def load_truthfulqa_canonical(zip_path: str):\n",
        "    out = []\n",
        "    if not os.path.exists(zip_path):\n",
        "        print(f'[TruthfulQA] Zip not found: {zip_path}')\n",
        "        return out\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "            z.extractall(os.path.join(RAW_DIR, 'truthfulqa_extracted'))\n",
        "    except Exception as e:\n",
        "        print('[TruthfulQA] Extraction failed:', e)\n",
        "        return out\n",
        "\n",
        "    csvs = []\n",
        "    for root, _, files in os.walk(os.path.join(RAW_DIR, 'truthfulqa_extracted')):\n",
        "        for fn in files:\n",
        "            if fn.lower().endswith('.csv'):\n",
        "                csvs.append(os.path.join(root, fn))\n",
        "\n",
        "    gen_candidates = [p for p in csvs if 'generation' in os.path.basename(p).lower()]\n",
        "    mc_candidates  = [p for p in csvs if 'multiple'   in os.path.basename(p).lower()]\n",
        "\n",
        "    if not gen_candidates and csvs:\n",
        "        gen_candidates = [csvs[0]]\n",
        "    if len(csvs) > 1 and not mc_candidates:\n",
        "        mc_candidates = [csvs[1]]\n",
        "\n",
        "    # generation CSVs\n",
        "    for p in gen_candidates:\n",
        "        try:\n",
        "            df = pd.read_csv(p)\n",
        "        except Exception as e:\n",
        "            print('[TruthfulQA] Could not read:', p, e)\n",
        "            continue\n",
        "        q_col = ci_get(df, 'question', 'prompt', 'query')\n",
        "        a_col = ci_get(df, 'best_answer', 'correct_answer', 'answer')\n",
        "        cat   = ci_get(df, 'category', 'topic', 'domain')\n",
        "        incor = ci_get(df, 'incorrect_answers', 'incorrect', 'distractors')\n",
        "        src   = ci_get(df, 'source', 'url', 'reference')\n",
        "        if q_col is None or a_col is None:\n",
        "            print('[TruthfulQA] Skipping generation CSV with missing cols:', p)\n",
        "            continue\n",
        "        for i, r in df.iterrows():\n",
        "            out.append({\n",
        "                'dataset': 'truthfulqa_gen',\n",
        "                'id': to_str_id(('truth_gen', p, i)),\n",
        "                'query': clean_text(r[q_col]),\n",
        "                'answer': clean_text(r[a_col]),\n",
        "                'contexts': [],\n",
        "                'label': None,\n",
        "                'meta': {\n",
        "                    'category': clean_text(r[cat]) if cat else None,\n",
        "                    'incorrect_answers': clean_text(r[incor]) if incor else None,\n",
        "                    'source': clean_text(r[src]) if src else None,\n",
        "                    'source_csv': os.path.basename(p)\n",
        "                }\n",
        "            })\n",
        "\n",
        "    # multiple-choice CSVs\n",
        "    for p in mc_candidates:\n",
        "        try:\n",
        "            df = pd.read_csv(p)\n",
        "        except Exception as e:\n",
        "            print('[TruthfulQA] Could not read:', p, e)\n",
        "            continue\n",
        "        q_col = ci_get(df, 'question', 'prompt', 'query')\n",
        "        cat   = ci_get(df, 'category', 'topic', 'domain')\n",
        "        src   = ci_get(df, 'source', 'url', 'reference')\n",
        "        mc_cols = [c for c in df.columns if 'mc' in c.lower() and ('target' in c.lower() or 'targets' in c.lower() or 'choice' in c.lower())]\n",
        "        if not mc_cols:\n",
        "            mc_cols = [c for c in df.columns if 'option' in c.lower() or 'choice' in c.lower()]\n",
        "        if q_col is None or not mc_cols:\n",
        "            print('[TruthfulQA] Skipping MC CSV with missing cols:', p)\n",
        "            continue\n",
        "        for i, r in df.iterrows():\n",
        "            choices = {c: r.get(c, None) for c in mc_cols}\n",
        "            out.append({\n",
        "                'dataset': 'truthfulqa_mc',\n",
        "                'id': to_str_id(('truth_mc', p, i)),\n",
        "                'query': clean_text(r[q_col]),\n",
        "                'answer': None,\n",
        "                'contexts': [],\n",
        "                'label': None,\n",
        "                'meta': {\n",
        "                    'category': clean_text(r[cat]) if cat else None,\n",
        "                    'choices': choices,\n",
        "                    'source': clean_text(r[src]) if src else None,\n",
        "                    'source_csv': os.path.basename(p)\n",
        "                }\n",
        "            })\n",
        "    print(f'[TruthfulQA] Canonicalized rows: {len(out)}')\n",
        "    return out\n",
        "\n",
        "def load_hotpotqa_canonical(path: str, max_rows: int = 1000):\n",
        "    rows = []\n",
        "    if not os.path.exists(path):\n",
        "        print(f'[HotpotQA] Not found: {path} (optional)')\n",
        "        return rows\n",
        "    try:\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "    except Exception as e:\n",
        "        print('[HotpotQA] Could not read dev.json:', e)\n",
        "        return rows\n",
        "    for i, ex in enumerate(data[:max_rows]):\n",
        "        contexts = []\n",
        "        for title, sents in ex.get('context', []):\n",
        "            contexts.append({'doc_id': title, 'text': ' '.join(sents)})\n",
        "        rows.append({\n",
        "            'dataset': 'hotpotqa',\n",
        "            'id': ex.get('_id', to_str_id(i)),\n",
        "            'query': clean_text(ex.get('question')),\n",
        "            'answer': clean_text(ex.get('answer')),\n",
        "            'contexts': contexts,\n",
        "            'label': None,\n",
        "            'meta': {'supporting_facts': ex.get('supporting_facts', [])}\n",
        "        })\n",
        "    print(f'[HotpotQA] Canonicalized rows: {len(rows)}')\n",
        "    return rows"
      ],
      "id": "Im-q_coMBxiL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaNcVjxdBxiL"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "all_rows = []\n",
        "fever_rows     = load_fever_canonical(FEVER_JSONL)\n",
        "truthful_rows  = load_truthfulqa_canonical(TRUTH_ZIP)\n",
        "hotpot_rows    = load_hotpotqa_canonical(HOTPOT_DEV, max_rows=1000)\n",
        "\n",
        "if fever_rows: all_rows.extend(fever_rows)\n",
        "if truthful_rows: all_rows.extend(truthful_rows)\n",
        "if hotpot_rows: all_rows.extend(hotpot_rows)\n",
        "\n",
        "print('Counts:', {'fever': len(fever_rows), 'truthfulqa': len(truthful_rows), 'hotpotqa': len(hotpot_rows), 'total': len(all_rows)})\n",
        "if all_rows:\n",
        "    write_jsonl(all_rows, CANONICAL_OUT)\n",
        "    print('Wrote canonical JSONL →', CANONICAL_OUT)"
      ],
      "id": "HaNcVjxdBxiL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgJVZ3lmBxiM"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def length_stats(texts):\n",
        "    lengths = [len(str(t).split()) for t in texts if isinstance(t, str)]\n",
        "    if not lengths: return {'count':0,'mean':0,'median':0,'p95':0}\n",
        "    arr = np.array(lengths)\n",
        "    return {'count': int(len(arr)), 'mean': float(arr.mean()), 'median': float(np.median(arr)), 'p95': float(np.percentile(arr, 95))}\n",
        "\n",
        "stats = []\n",
        "if fever_rows:\n",
        "    s = length_stats([r['query'] for r in fever_rows]); s.update({'dataset':'FEVER'}); stats.append(s)\n",
        "if truthful_rows:\n",
        "    s = length_stats([r['query'] for r in truthful_rows]); s.update({'dataset':'TruthfulQA'}); stats.append(s)\n",
        "if hotpot_rows:\n",
        "    s = length_stats([r['query'] for r in hotpot_rows]); s.update({'dataset':'HotpotQA'}); stats.append(s)\n",
        "\n",
        "summary_df = pd.DataFrame(stats)[['dataset','count','mean','median','p95']]\n",
        "summary_df"
      ],
      "id": "bgJVZ3lmBxiM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhdbhepNBxiM"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# FEVER plots\n",
        "if fever_rows:\n",
        "    df = pd.DataFrame(fever_rows)\n",
        "    plt.figure(); df['label'].value_counts(dropna=False).plot(kind='bar')\n",
        "    plt.title('FEVER label distribution'); plt.xlabel('Label'); plt.ylabel('Count'); plt.tight_layout(); plt.show()\n",
        "\n",
        "    lengths = df['query'].fillna('').apply(lambda s: len(str(s).split()))\n",
        "    plt.figure(); plt.hist(lengths, bins=40)\n",
        "    plt.title('FEVER claim length (words)'); plt.xlabel('Words'); plt.ylabel('Frequency'); plt.tight_layout(); plt.show()\n",
        "\n",
        "    vals = np.sort(lengths.values); cdf = np.arange(1, len(vals)+1) / len(vals)\n",
        "    plt.figure(); plt.plot(vals, cdf)\n",
        "    plt.title('FEVER claim length CDF'); plt.xlabel('Words'); plt.ylabel('CDF'); plt.tight_layout(); plt.show()\n",
        "\n",
        "    df[['id','query','label']].head(3)"
      ],
      "id": "DhdbhepNBxiM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVlNlo3vBxiM"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TruthfulQA plots\n",
        "if truthful_rows:\n",
        "    df = pd.DataFrame(truthful_rows)\n",
        "    cats = df['meta'].apply(lambda m: (m or {}).get('category', None))\n",
        "    top = cats.value_counts(dropna=True).head(15)\n",
        "    if not top.empty:\n",
        "        plt.figure(); top.plot(kind='bar')\n",
        "        plt.title('TruthfulQA categories (top 15)'); plt.xlabel('Category'); plt.ylabel('Count'); plt.tight_layout(); plt.show()\n",
        "\n",
        "    q_lengths = df['query'].fillna('').apply(lambda s: len(str(s).split()))\n",
        "    plt.figure(); plt.hist(q_lengths, bins=40)\n",
        "    plt.title('TruthfulQA question length (words)'); plt.xlabel('Words'); plt.ylabel('Frequency'); plt.tight_layout(); plt.show()\n",
        "\n",
        "    vals = np.sort(q_lengths.values); cdf = np.arange(1, len(vals)+1) / len(vals)\n",
        "    plt.figure(); plt.plot(vals, cdf)\n",
        "    plt.title('TruthfulQA question length CDF'); plt.xlabel('Words'); plt.ylabel('CDF'); plt.tight_layout(); plt.show()\n",
        "\n",
        "    df[['id','query','answer']].head(3)"
      ],
      "id": "tVlNlo3vBxiM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-Ab7zpCBxiM"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# HotpotQA plots (if present)\n",
        "if hotpot_rows:\n",
        "    df = pd.DataFrame(hotpot_rows)\n",
        "    lengths = df['query'].fillna('').apply(lambda s: len(str(s).split()))\n",
        "    plt.figure(); plt.hist(lengths, bins=40)\n",
        "    plt.title('HotpotQA question length (words)'); plt.xlabel('Words'); plt.ylabel('Frequency'); plt.tight_layout(); plt.show()\n",
        "\n",
        "    num_ctx = df['contexts'].apply(lambda cs: len(cs) if isinstance(cs, list) else 0)\n",
        "    plt.figure(); plt.hist(num_ctx, bins=40)\n",
        "    plt.title('HotpotQA #contexts per question'); plt.xlabel('#contexts'); plt.ylabel('Frequency'); plt.tight_layout(); plt.show()\n",
        "\n",
        "    df[['id','query','answer']].head(3)"
      ],
      "id": "L-Ab7zpCBxiM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4TS4XtwBxiM"
      },
      "source": [
        "## Improved Retrieval for TruthfulQA (TF‑IDF over Questions)"
      ],
      "id": "Y4TS4XtwBxiM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmsNHsFBBxiM"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "truth_q_rows = [r for r in truthful_rows if r['dataset'].startswith('truthfulqa')]\n",
        "\n",
        "if truth_q_rows:\n",
        "    questions = [r['query'] or '' for r in truth_q_rows]\n",
        "    tfidf = TfidfVectorizer(ngram_range=(1,2), stop_words='english')\n",
        "    X = tfidf.fit_transform(questions)\n",
        "\n",
        "    def search_truthfulqa(query: str, k: int = 5):\n",
        "        qv = tfidf.transform([query])\n",
        "        sims = linear_kernel(qv, X).ravel()  # cosine because TF-IDF vectors are L2-normalized by default\n",
        "        topk = sims.argsort()[::-1][:k]\n",
        "        results = []\n",
        "        for idx in topk:\n",
        "            r = truth_q_rows[idx]\n",
        "            results.append({\n",
        "                'score': float(sims[idx]),\n",
        "                'id': r['id'],\n",
        "                'query': r['query'],\n",
        "                'answer': r.get('answer'),\n",
        "                'category': (r.get('meta') or {}).get('category'),\n",
        "                'source': (r.get('meta') or {}).get('source')\n",
        "            })\n",
        "        return results\n",
        "else:\n",
        "    def search_truthfulqa(query: str, k: int = 5):\n",
        "        return []\n",
        "\n",
        "print('TruthfulQA TF-IDF index built:', len(truth_q_rows), 'questions')"
      ],
      "id": "wmsNHsFBBxiM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Bqpl9L4BxiM"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "demo_q = \"What happens to you if you eat watermelon seeds?\"\n",
        "print(\"DEMO query:\", demo_q)\n",
        "for item in search_truthfulqa(demo_q, k=5):\n",
        "    print(f\"score={item['score']:.3f} | Q={item['query']}\")\n",
        "    print(f\"  → ANSWER: {item['answer']}\")\n",
        "    if item.get('source'):\n",
        "        print(f\"  source: {item['source']}\")\n",
        "    print()"
      ],
      "id": "7Bqpl9L4BxiM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lng-XmZ7BxiM"
      },
      "source": [
        "### ✅ Why this works better\n",
        "- We index **questions**, not answers.  \n",
        "- We use **TF‑IDF with bigrams + stopword removal**, which matches key phrases like *“watermelon seeds”* and avoids generic noise like *“Nothing happens…”*.  \n",
        "- For HotpotQA (when added), you’ll switch to **document/context retrieval**, which is true RAG. For Workbook‑1, this question‑level retrieval is sufficient to show the pipeline is working and accurate on Generic datasets.\n",
        "\n",
        "**Next steps**:  \n",
        "- Swap TF‑IDF with **embedding retrieval (BGE)**, add **reranker**, and then the **guardrails** (NLI + judge + trust score)."
      ],
      "id": "lng-XmZ7BxiM"
    }
  ]
}